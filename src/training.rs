use std::time::{Duration, Instant};

use burn::{config::Config, optim::{AdamConfig, GradientsParams, GradientsAccumulator, Optimizer}, tensor::{backend::{AutodiffBackend, Backend}, Data, Tensor, ops::TensorOps, Float, Int}, data::{dataloader::{self, DataLoaderBuilder}, dataset::SqliteDataset, self}, nn::loss::{CrossEntropyLoss, BinaryCrossEntropyLoss, BinaryCrossEntropyLossConfig}, record::CompactRecorder};
use burn::module::Module;
use image::Rgb;

use chrono::Local;

use crate::{models::{GeneratorConfig, DiscriminatorConfig, Discriminator}, data_loader::{ImageBatcher, make_image_dataset}, image::{IMAGE_HEIGHT, IMAGE_WIDTH}};



#[derive(Config)]
pub struct TrainingConfig {
    pub generator: GeneratorConfig,
    pub discriminator: DiscriminatorConfig,
    pub optimizer: AdamConfig,
    #[config(default = 4)]
    pub num_epochs: usize,
    #[config(default = 128)]
    pub batch_size: usize,
    #[config(default = 2)]
    pub num_workers: usize,
    #[config(default = 4242)]
    pub seed: u64,
    #[config(default = 0.0002)]
    pub learning_rate: f64,
}

pub fn train<B: AutodiffBackend>(artifact_dir: &str, config: TrainingConfig, device: B::Device) {
    println!("Starting Training Setup.");

    std::fs::create_dir_all(artifact_dir).ok();
    config
        .save(format!("{artifact_dir}/config.json"))
        .expect("Config should be saved successfully");

    B::seed(config.seed);

    let mut generator = config.generator.init::<B>();
    let mut discriminator = config.discriminator.init::<B>();
    let mut optimizer_dis = config.optimizer.init();
    let mut optimizer_gen = config.optimizer.init();

    let image_batcher = ImageBatcher::<B>::new(device.clone());
    let dataloader = DataLoaderBuilder::new(image_batcher)
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(make_image_dataset());

    let progress_image_latents = Tensor::<B,1,Float>::ones([config.generator.latent_vector_size]);

    println!("Generator Sizes:");
    generator.forward_print_sizes(progress_image_latents.clone().reshape([1,config.generator.latent_vector_size])).reshape([3,IMAGE_WIDTH, IMAGE_HEIGHT]);


    const RING_BUFFER_SIZE: usize = 20;
    let mut time_ring_buffer = [Duration::from_secs(0) ; RING_BUFFER_SIZE];
    let mut num_in_ring_buffer = 0;
    let mut ring_buffer_idx = 0;

    println!("Finished Training Setup.");

    // Custom Training Loop for GANs
    for epoch in 1..config.num_epochs + 1{
        for (iteration, batch) in dataloader.iter().enumerate(){
            let iter_start_time = Instant::now();

            // Update Discriminator Network
            let mut accumulated_gradients = GradientsAccumulator::<Discriminator<B>>::new();
            
            // Generate Loss from Real Images
            let all_real_discriminator_output = discriminator.forward(batch.images);
            let generator_loss_0 = BinaryCrossEntropyLossConfig::new().init().forward(all_real_discriminator_output.clone(), Tensor::<B,1,Int>::ones([config.batch_size]));
            let grads_real = generator_loss_0.backward();
            let grads_real = GradientsParams::from_grads(grads_real, &discriminator);
            accumulated_gradients.accumulate(&discriminator, grads_real);

            // Generate Loss from Fake Images generated by Generator
            let noise_for_generator: Tensor<B, 2> = Tensor::random([config.batch_size, config.generator.latent_vector_size], burn::tensor::Distribution::Default);
            let fake_images = generator.forward(noise_for_generator);
            let all_fake_discriminator_output_1 = discriminator.forward(fake_images.clone().detach());
            let generator_loss_1 = BinaryCrossEntropyLossConfig::new().init().forward(all_fake_discriminator_output_1.clone(), Tensor::<B,1,Int>::zeros([config.batch_size]));
            let grads_fake = generator_loss_1.backward();
            let grads_fake = GradientsParams::from_grads(grads_fake, &discriminator);
            accumulated_gradients.accumulate(&discriminator, grads_fake);

            // Apply Loss
            let total_grads = accumulated_gradients.grads();
            discriminator = optimizer_dis.step(config.learning_rate, discriminator, total_grads);

            // Update Generator Network
            // Fake labels are real labels for generator cost: See https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html
            let all_fake_labels = Tensor::<B,1,Int>::ones([config.batch_size]);
            let all_fake_discriminator_output_2 = discriminator.forward(fake_images);
            let generator_loss_2 = BinaryCrossEntropyLossConfig::new().init().forward(all_fake_discriminator_output_2.clone(), all_fake_labels);
            let generator_loss_grads = generator_loss_2.backward();
            let grads = GradientsParams::from_grads(generator_loss_grads, &generator);
            generator = optimizer_gen.step(config.learning_rate, generator, grads);

            let end_iter_time = Instant::now();

            num_in_ring_buffer = if num_in_ring_buffer < RING_BUFFER_SIZE {
                num_in_ring_buffer + 1
            } else {
                num_in_ring_buffer
            };
            time_ring_buffer[ring_buffer_idx] = end_iter_time - iter_start_time;
            ring_buffer_idx = (ring_buffer_idx + 1) % RING_BUFFER_SIZE;

            // Reporting
            if iteration % 20 == 0{
                let mut total_last_8_time = Duration::from_secs(0);
                for i in 0..num_in_ring_buffer{
                    total_last_8_time += time_ring_buffer[i];
                }
                println!(
                    "[{}]: [Train - Epoch {} - Iteration {}] Loss Gen {:.3} | Loss Dis {:.3} | D(x): {:.3} | D(G(z)): {:.3} / {:.3} - Last {RING_BUFFER_SIZE} Iters took: {:.2}s, {:.2}s per Iteration on avg",
                    Local::now(),
                    epoch,
                    iteration,
                    generator_loss_2.sum().into_scalar(),
                    generator_loss_0.add(generator_loss_1).sum().into_scalar(),
                    all_real_discriminator_output.mean().into_scalar(),
                    all_fake_discriminator_output_1.mean().into_scalar(),
                    all_fake_discriminator_output_2.mean().into_scalar(),
                    total_last_8_time.as_secs_f32(),
                    total_last_8_time.as_secs_f32() / num_in_ring_buffer as f32,
                );
            }
            if iteration % 40 == 0{
                let image_generated = generator.forward(progress_image_latents.clone().reshape([1,config.generator.latent_vector_size])).reshape([3,IMAGE_WIDTH, IMAGE_HEIGHT]);
                tensor_to_image(&format!("gan_progress_output/{epoch}-{iteration}-progress.png"), image_generated);
            }
            if iteration % 40 == 0{
                generator
                    .clone()
                    .save_file(format!("{artifact_dir}/generator-{epoch}-{iteration}"), &CompactRecorder::new())
                    .expect("Generator model should be saved successfully");
                discriminator
                    .clone()
                    .save_file(format!("{artifact_dir}/discriminator-{epoch}-{iteration}"), &CompactRecorder::new())
                    .expect("Discriminator model should be saved successfully");
                println!("[{}]: Successfully Saved Models", Local::now());
            }
        }
    }
}

pub fn tensor_to_image<B: Backend>(path: &str, tensor: Tensor<B, 3>){
    let data: Data<f32, 3> = tensor.into_data().convert();
        
    let image_data: Vec<u8> = data.value.iter().map(|pix_chan| ((pix_chan + 0.5) * 255.0) as u8).collect();

    let new_image: image::ImageBuffer<Rgb<u8>, Vec<u8>> = image::ImageBuffer::from_raw(IMAGE_WIDTH as u32, IMAGE_HEIGHT as u32, image_data).unwrap();
    new_image.save(path).unwrap();
}